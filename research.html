<!DOCTYPE html>
<!-- saved from url=(0036)https://www.cc.gatech.edu/~aedwards/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Ioannis Exarchos</title>
    
    <meta name="description" content="">
        <meta name="keywords" content="">
            <link href="./Ioannis Exarchos_files/css" rel="stylesheet">
                <script async="" src="./Ioannis Exarchos_files/analytics.js"></script><script src="./Ioannis Exarchos_files/jquery.min.js"></script>
                <script src="./Ioannis Exarchos_files/config.js"></script>
                <script src="./Ioannis Exarchos_files/skel.min.js"></script>
                <noscript>
                    <link rel="stylesheet" href="css/skel-noscript.css" />
                    <link rel="stylesheet" href="css/style.css" />
                    <link rel="stylesheet" href="css/style-desktop.css" />
                </noscript>
                <!--[if lte IE 9]><link rel="stylesheet" href="css/ie9.css" /><![endif]-->
                <!--[if lte IE 8]><script src="js/html5shiv.js"></script><link rel="stylesheet" href="css/ie8.css" /><![endif]-->
                <!--[if lte IE 7]><link rel="stylesheet" href="css/ie7.css" /><![endif]-->
                <script>
                    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                     (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                     m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
                     
                     ga('create', 'UA-61243618-1', 'auto');
                     ga('send', 'pageview');
                     
                    </script><style type="text/css">html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td,article,aside,canvas,details,embed,figure,figcaption,footer,header,hgroup,menu,nav,output,ruby,section,summary,time,mark,audio,video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline;}article,aside,details,figcaption,figure,footer,header,hgroup,menu,nav,section{display:block;}body{line-height:1;}ol,ul{list-style:none;}blockquote,q{quotes:none;}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none;}table{border-collapse:collapse;border-spacing:0;}body{-webkit-text-size-adjust:none}</style><style type="text/css">.container{width:1200px !important;margin: 0 auto;}</style><style type="text/css">.\31 2u{width:100%}.\31 1u{width:91.6666666667%}.\31 0u{width:83.3333333333%}.\39 u{width:75%}.\38 u{width:66.6666666667%}.\37 u{width:58.3333333333%}.\36 u{width:50%}.\35 u{width:41.6666666667%}.\34 u{width:33.3333333333%}.\33 u{width:25%}.\32 u{width:16.6666666667%}.\31 u{width:8.3333333333%}.\31 u,.\32 u,.\33 u,.\34 u,.\35 u,.\36 u,.\37 u,.\38 u,.\39 u,.\31 0u,.\31 1u,.\31 2u{float:left;-moz-box-sizing:border-box;-webkit-box-sizing:border-box;-o-box-sizing:border-box;-ms-box-sizing:border-box;box-sizing:border-box}.\-11u{margin-left:91.6666666667%}.\-10u{margin-left:83.3333333333%}.\-9u{margin-left:75%}.\-8u{margin-left:66.6666666667%}.\-7u{margin-left:58.3333333333%}.\-6u{margin-left:50%}.\-5u{margin-left:41.6666666667%}.\-4u{margin-left:33.3333333333%}.\-3u{margin-left:25%}.\-2u{margin-left:16.6666666667%}.\-1u{margin-left:8.3333333333%}.row.flush{margin-left:0}.row.flush>*{padding:0!important}</style><style type="text/css">.row>*{padding:50px 0 0 50px}.row+.row>*{padding-top:50px}.row{margin-left:-50px}.row.half>*{padding:25px 0 0 25px}.row.half+.row.half>*{padding-top:25px}.row.half{margin-left:-25px}.row.double>*{padding:100px 0 0 100px}.row.double+.row.double>*{padding-top:100px}.row.double{margin-left:-100px}</style><style type="text/css">.row:after{content:'';display:block;clear:both;height:0}.row:first-child>*{padding-top:0}.row>*{padding-top:0}</style><style type="text/css">.not-desktop{display:none}.only-mobile,.only-1000px{display:none}</style><link rel="stylesheet" type="text/css" href="./Ioannis Exarchos_files/style.css"><link rel="stylesheet" type="text/css" href="./Ioannis Exarchos_files/style-desktop.css"></head>

<body>
    
    <!-- Nav -->
    <nav id="nav">
        <ul>
            <li><a href="index.html">About</a></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="cv.pdf">CV</a></li>
            <li><a href="news.html">News</a></li>
            <li><a href="contact.html">Contact</a></li>
        </ul>
    </nav>
    
    <!-- Projects -->
    <div class="wrapper wrapper-style2" align="justify">
        <article id="work">
            <header>
                <h2>Selected Research</h2>
                <p align="justify" style="padding: 0 100px">
                My research has largely focused on stochastic control, reinforcement learning, ML applications in neuroscience, and some differential game theory. You can view my doctoral dissertation <a href="https://smartech.gatech.edu/handle/1853/59263">here</a>!
                </p>
                <hr>
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Policy Transfer via Kinematic Domain Randomization and Adaptation <a href="https://arxiv.org/abs/2011.01891">[paper]</a> <!-- <a href="none">[code]</a> <a href="https://sites.google.com/view/qss-paper"> [videos]</a>--></b>
                    <br>work with Yifeng Jiang, Wenhao Yu, and C. Karen Liu
                    <br>
                    <br><video width=40% height=40% controls>
                        <source src="paper_images/ICRA2021_video.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                            </video>
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                Transferring reinforcement learning policies trained in physics simulation to the real hardware remains a challenge, known as the “sim-to-real” gap. Domain random-ization is a simple yet effective technique to address dynamics discrepancies across source and target domains, but its successgenerally depends on heuristics and trial-and-error. In this work we investigate the impact of randomized parameter selection on policy transferability across different types of domain discrepancies. Contrary to common practice in which kinematic parameters are carefully measured while dynamic parameters are randomized, we found that virtually randomizing kinematic parameters (e.g., link lengths) during training in simulation generally outperforms dynamic randomization. Based on this finding, we introduce a new domain adaptation algorithm that utilizes simulated kinematic parameters variation. Our algorithm, Multi-Policy Bayesian Optimization, trains an en- semble of universal policies conditioned on virtual kinematic parameters and efficiently adapts to the target environment using a limited number of target domain rollouts. We showcase our findings on a simulated quadruped robot in five different target environments covering different aspects of domain discrepancies.
                <!--<br><br>This work was accepted into <a href="https://icml.cc/Conferences/2020">ICML 2020</a>.-->
                </p>
                <hr>
                <!--
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Imitating Latent Policies from Observation <a href="https://arxiv.org/abs/1805.07914">[paper]</a> <a href="https://github.com/ashedwards/ILPO">[code]</a></b>
                    <br>Ashley D. Edwards, Himanshu Sahni, Yannick Schroecker, Charles L. Isbell
                    <br>
                    <br><img src="paper_images/ilpo2.gif" height=20% width=20%>    <img src="paper_images/cartpole_50.gif" height=20% width=20%>
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                In this paper, we describe a novel approach to
                imitation learning that infers latent policies directly from state observations. We introduce a
                method that characterizes the causal effects of latent actions on observations while simultaneously
                predicting their likelihood. We then outline an
                action alignment procedure that leverages a small
                amount of environment interactions to determine
                a mapping between the latent and real-world actions. We show that this corrected labeling can
                be used for imitating the observed behavior, even
                though no expert actions are given. We evaluate our approach within classic control environments and a platform game and demonstrate that
                it performs better than standard approaches.
                
                <br><br>This work was accepted into <a href="https://icml.cc/Conferences/2019">ICML 2019</a>.
                </p>
                <hr>
            -->

                
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Learning Deep Optimal Control Policies using Forward and Backward SDEs 
                <a href="https://arxiv.org/abs/2006.11992">[paper 1]</a>
                <a href="https://arxiv.org/abs/2009.01196">[paper 2]</a>
                <a href="http://www.roboticsproceedings.org/rss15/p70.pdf">[paper 3]</a>
                <a href="https://doi.org/10.1109/CDC40024.2019.9028871">[paper 4]</a></b>
                    <br>work with Marcus Pereira, Ziyi Wang, and Evangelos Theodorou
                    <br>
                    <br><img src="paper_images/DFBSDE1.png" height=20% width=20%>        <img src="paper_images/DFBSDE2.png" height=22% width=22%>  <img src="paper_images/DFBSDE3.png" height=18% width=18%> 
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                This line of work represents a deep learning extension of the stochastic optimal control framework via Forward and Backward Stochastic Differential Equations (FBSDEs), developed in my doctoral dissertation (see further down bellow).  The mathematical formulation of a Stochastic Optimal Control (SOC) problem leads to a nonlinear PDE, the Hamilton-Jacobi-Bellman PDE. This motivates algorithmic development for stochastic control that combine elements of PDE theory with deep learning. The transition from a PDE formulation to a trainable neural network is done via the concept of a system of Forward-Backward Stochastic Differential Equations (FBSDEs). Specifically, certain PDE solutions are linked to solutions of FBSDEs, which are the stochastic equivalent of a two-point boundary value problem and can be solved using a suitably defined neural network architecture. This is known in the literature as the deep FBSDE approach; the FBSDEs are then discretized over time and solved on a neural network graph. A system of FBSDEs is shown in the left figure, an example of a deep neural network architecture used to solve this system is shown in the middle. We demonstrated the scalability and applicability of the deep FBSDE controller in a finance problem, namely a 101-dimensional continuous-time stock portfolio optimization problem where the objective is to outperform the market average (see right figure).
                

                
                </p>
                <hr>

                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Classification of Sleep Stage and Cataplexy in Narcoleptic Mice using Supervised and Unsupervised Machine Learning <a href="https://doi.org/10.1093/sleep/zsz272">[paper]</a></b>
                    <br>I. Exarchos, A. A. Rogers, L. M. Aiani, R. E. Gross, G. D. Clifford, N. P. Pedersen, and J. T.
Willie
                    <br>
                    <br><img src="paper_images/SLEEP1.png" height=20% width=20%>          <img src="paper_images/SLEEP2.png" height=22% width=22%> <br> <img src="paper_images/SLEEP3.png" height=18% width=18%> 
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
We optimize two machine-learning approaches, supervised and unsupervised, for automated scoring of behavioral states in orexin/
ataxin-3 transgenic mice, a validated model of narcolepsy type 1, and additionally test them on wild-type mice. For the
supervised approach, we employ a deep convolutional neural network architecture that is trained on expert-labeled segments of
wake, non-REM sleep, and REM sleep in EEG/EMG time series data. The resulting trained classifier is then used to infer on the labels of previously unseen data. For the unsupervised approach, we leverage data dimensionality reduction and clustering techniques. Both approaches successfully score EEG/EMG data, achieving mean accuracies of 95% and 91%, respectively, in narcoleptic mice, and accuracies of 93% and 89%, respectively, in wild-type mice. This work is, to the best of our knowledge, the first set of algorithms created to specifically identify pathological sleep in narcoleptic mice. Currently available sleep-scoring algorithms are trained on wild-type animals with normal sleep/wake behavior and exhibit low accuracies
for scoring pathological sleep. Therefore, this set of tools can greatly facilitate and expedite
behavioral-state-scoring in narcoleptic mice, and thus is a valuable asset for sleep research labs.
                </p>
                <hr>
                
                
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Stochastic Optimal Control using Forward and Backward Stochastic Differential Equations
                 <a href="https://smartech.gatech.edu/handle/1853/59263">[Thesis]</a>
                <a href="https://www.sciencedirect.com/science/article/pii/S0005109817304740">[paper 1]</a>
                <a href="https://doi.org/10.1016/j.sysconle.2018.06.005">[paper 2]</a>
                <a href="https://link.springer.com/article/10.1007/s13235-018-0268-4">[paper 3]</a>
                <a href="https://doi.org/10.2514/1.G003598">[paper 4]</a></b>
                    <br>work with Evangelos Theodorou and Panagiotis Tsiotras
                    <br>
                    <br><img src="paper_images/FBSDE1.png" height=22% width=22%>           <img src="paper_images/FBSDE2.png" height=20% width=20%>  <img src="paper_images/FBSDE3.png" height=18% width=18%> 
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
In this line of work, we utilized tools from control theory, stochastic calculus, and machine learning, to propose a mathematical framework capable of addressing a large variety of Stochastic Optimal Control (SOC) problems, as well as stochastic differential games.  In general, the mathematical formulation of a SOC problem leads to a nonlinear partial differential equation (PDE), known as the Hamilton-Jacobi-Bellman (HJB) PDE. Furthermore, by virtue of some results in stochastic calculus, certain PDE solutions are linked to solutions of systems of Forward and Backward Stochastic Differential Equations (FBSDEs, left figure); the latter can be seen as a stochastic equivalent of a two-point boundary value problem. Thus, rather than solving a PDE directly, one can solve its corresponding system of FBSDEs instead -- this is done numerically using sampling and function approximation techniques.  The result is a novel sampling-based algorithm, which leads to a nonlinear optimal feedback control law. This controller can be utilized for a variety of control tasks (example: cart-pole in middle figure) and differential games. This is in contrast to most available algorithms within optimal control (e.g., DDP/iLQR, Path Integral control, collocation, multiple shooting etc.) which perform local trajectory optimization and need to close the loop by recalculating the solution at each instant of time, thus requiring heavy online computation during deployment. Applying the algorithm on the fuel-optimal Mars landing problem demonstrated that a deterministic optimal control law, even if it is applied in a closed-loop fashion (i.e., by recalculating the control at each instant of time), does not mitigate the risk induced by the environmental disturbances, and leads to a high probability of crash (right figure). In contrast, using the proposed method, the probability of a crash can be controlled and can be made arbitrarily small, thus highlighting the importance of stochastic control whenever external disturbances are to be expected (right figure).
                
                </p>
                <hr>
                <p align="justify" style="padding: 0 100px">
                <b></b></p><center><b>Multi-agent Pursuit-Evasion Games 
                    <a href="http://link.springer.com/article/10.1007/s13235-014-0130-2/">[paper 1]</a>
                    <a href="https://ieeexplore.ieee.org/document/7040055/">[paper 2]</a>
                    <a href="https://arc.aiaa.org/doi/10.2514/6.2016-2100">[paper 3]</a></b>

                    <br>work with Meir Pachter and Panagiotis Tsiotras
                    <br>
                    <br><img src="paper_images/PE1.png" height=20% width=20%>           <img src="paper_images/PE2.png" height=20.5% width=20.5%>  <img src="paper_images/PE3.png" height=20% width=20%> 
                    <br></center><p></p><p align="justify" style="padding: 0 100px">
                We published a couple papers on multi-agent pursuit evasion games. Using the differential game theory developed by R. Isaacs, we analyzed a game in which a less maneuverable (but potentially faster) agent is pursued by an agile agent. We determined initial conditions that lead to capture and obtained optimal controls and capture times. Using these results, we then developed a collision avoidance framework that considers the worst case of a malicious opponent that is attempting to cause a collision. The results were published in the Journal of Dynamic Games and Applications, the IEEE Conference on Decision and Control, as well as the AIAA SciTech Guidance, Navigation, and Control Conference.
                <!--<br><br>This work was accepted into <a href="http://rldm.org/rldm2015/" target="_blank">RLDM 2015</a>.-->
                </p>
                <hr>

                
                <ul id="copyright">
                    <li>Design: <a href="http://html5up.net/">HTML5 UP</a></li>
                </ul>
                </footer>
</body></html>

